---
title: "ADCIRC Reanalysis"
author: "Brian N. White"
date: "2/19/2022"
output:
  html_document: default
  pdf_document: default
---

```{r, include = F}
knitr::opts_chunk$set(fig.align = 'center', warning = F, message = F, tidy=TRUE)
```


```{r, include = F}
# data wrangling & visualization
library(tidyverse)
library(reshape2)
library(GGally)
library(Matrix)

# supplementary distributions
library(mvtnorm)

# extreme value analysis
library(extRemes)

# working with spatial data
library(maps)
library(ggmap)
library(geosphere)
```


### Data Description

1. df_meta_region3.csv: Contains identifying information for each NOAA sea-level observation station (i.e. lat/lon coordinates, state, node, name, id). There are 26 rows and 6 columns. Each row is a station, each column an identifying feature. The stations range the majority of the U.S. East Coast but the focus is FEMA region 3.

2. ADC_POST_Detided_region3.csv: Contains hourly ADC_POST_Detided time-series data for each NOAA sea-level observation station. The time-series spans the 40-year period from 1979-01-01 01:00:00 to 2019-12-31 23:00:00. There are 359400 rows and 28 columns. Each row is an hourly observation and, except for TIME and year, each column is a station,

3. ADC_NOAA_OBS_region3.csv: Identical structure as ADC_POST_Detided_region3 but for hourly NOAA_OBS_Detided time-series data.

### Data Wrangling

```{r import and clean data, warning = F, message = F}
# load data
df_meta_region3 <- read.csv('data/df_meta_region3.csv')
ADC_POST_Detided_region3 <- read.csv('data/ADC_POST_Detided_region3.csv')
NOAA_OBS_Detided_region3 <- read.csv('data/NOAA_OBS_Detided_region3.csv')
```


```{r clean data, warning = F, message = F}
# Lewisetta was excluded from ADC_POST_Detided before import so remove from df_meta_region3 and NOAA_OBS_Detided_region3 for consistency

# pull Lewisetta's station id
raw_Lewisetta_station_id <- df_meta_region3 %>%
    filter(stationname == !!'Lewisetta') %>%
    select(stationid) %>%
    pull()

# add X to beginning of station id to match column name in ADC_POST_Detided
Lewisetta_station_id <- paste('X', raw_Lewisetta_station_id, sep = '')

# remove Lewisetta from NOAA_OBS_Detided_region3
NOAA_OBS_Detided_region3 <- NOAA_OBS_Detided_region3 %>%
  select(-Lewisetta_station_id)

# remove Lewisetta from df_meta_region3
df_meta_region3 <- df_meta_region3 %>%
  filter(stationname != 'Lewisetta')

# Now, clean meta data
clean_metadata <- function(metadata) {

  # remove white-space from station names as this can lead to conflicts with ggplot
  metadata$stationname <- str_replace_all(metadata$stationname, " ", "")
  
  return(metadata)

}

# Then, clean the time-series
clean_timeseries <- function(timeseries, metadata) {
  
  # coerce time variable to POSIX class
   timeseries$TIME <- as.POSIXct(timeseries$TIME, format = "%Y-%m-%d %H:%M:%S")
  
  # create year and month variables from TIME column
  timeseries <- add_column(timeseries,  year = format(timeseries$TIME, '%Y'), month = format(timeseries$TIME,    '%m'), .after = 1)
  
  # rename columns with station name
  for(station in metadata$stationname){

    # extract station id
    raw_station_id <- metadata %>%
    filter(stationname == !!station) %>%
    select(stationid) %>%
    pull()

  # add an X to the beginning to match current variable names
  station_id <- paste('X', raw_station_id, sep = '')

  # rename columns for ease of use
  timeseries <- timeseries %>%
    rename(!!station := station_id)
  }
  
  return(timeseries)

}

# clean meta data and corresponding time-series under consideration
df_meta_region3 <- clean_metadata(df_meta_region3)
ADC_POST_Detided_region3 <- clean_timeseries(ADC_POST_Detided_region3, df_meta_region3)
NOAA_OBS_Detided_region3 <- clean_timeseries(NOAA_OBS_Detided_region3, df_meta_region3)
```

```{r check for missing values, warning = F}
# computes the number of NA values
sum_na <- function(x) sum(is.na(x))

# computes proportion of NA values
prop_na <- function(x) sum_na(x)/length(x)

# computes row index of NA values
which_na <- function(x) which(is.na(x) == T)

# if the proportion of NA values in a station (whether overall or within a year) exceeds this then it will be flagged for removal
overview_na <- function(timeseries, threshold, group_by_year = F) {
  
  if(group_by_year == F) {

        # computes the proportion of NA values as each station and then returns T or F if value exceeds threshold
        df1 <- timeseries %>%
            dplyr::select(-TIME, -year, -month) %>%
            summarise_all(prop_na) >  threshold
    
            # returns vector of station names to be excluded
            stations_to_exclude <- df_meta_region3$stationname[df1]
            return(stations_to_exclude)
            
  } else {
    
    # computes the proportion of NA values for each year for each station
    df1 <- timeseries %>%
            dplyr::select(-TIME, -month) %>%
            group_by(year) %>%
            summarise_all(prop_na) %>%
            dplyr::select(-year) > threshold
    
    # list which contains the years which should be removed for each station.
    years_to_exclude <- list()
    
    # fill the list
    for(station in 1:ncol(df1)) {
      
    df2 <- df1[, station]
    
            years_to_exclude_by_station <- unique(timeseries$year)[df2]
            years_to_exclude[[station]] <- years_to_exclude_by_station
    }
    
    names(years_to_exclude) <- df_meta_region3$stationname
    return(years_to_exclude)
  
  }
    
}

# returns stations that are missing more than 10% of their observations
overview_na(NOAA_OBS_Detided_region3, .1)
# for each station this returns the years that are missing more than 10% of their observations
overview_na(NOAA_OBS_Detided_region3, .1, T)
```

### Exploratory Data Analysis

*Interactive Map of Stations*

[kepler.gl](https://kepler.gl/demo/map?mapUrl=https://dl.dropboxusercontent.com/s/jdmoouleyema7z1/keplergl_s3tjnff.json)

```{r produce map of stations, warning = F}
states <- map_data("state")

east_coast <- states %>%
  filter(region %in% c('florida', 'georgia', 'south carolina', 'north carolina', 'virginia', 'district of columbia', 'pennsylvania', 'maryland', 'delaware', 'new jersey', 'connecticut', 'new york', 'rhode island', 'massachusetts', 'vermont', 'new hampshire', 'maine' ))

ggplot() + 
  geom_polygon(data = east_coast, aes(x = long, y = lat, group = group, fill = group), color = "white") +
  geom_point(data = df_meta_region3, aes(x = lon, y = lat), col = 'black', size = .8) +
  geom_text(data = df_meta_region3, aes(x = lon, y = lat, label = stationname), size = 2.5, col = 'red', check_overlap = T) +
  guides(fill = F) +
  theme_nothing()
```

*1. ADCIRC_POST_Detided*

```{r examine time-series, cache = T, message = F}
ADC_POST_Detided_region3 %>%
  #filter('1979-01-01 0:00:00' <= TIME, TIME <= '1980-01-01 00:00:00') %>%
  select(TIME, WrightsvilleBeach) %>%
  ggplot(aes(x = TIME, y = WrightsvilleBeach)) +
  geom_line(col = 'blue', size = 0.1, alpha= 0.5) +
  labs(x = 'year', y = 'sea-level (meters)', title = 'Wrightsville Beach')

# box plots and violin plots for each station
ADC_POST_Detided_region3 %>%
  select(-TIME, -year, -month) %>%
  melt() %>%
  ggplot(aes(x = value, y = variable)) +
  geom_boxplot(outlier.size = 0.1)

# density plots for each station
ADC_POST_Detided_region3 %>%
  melt(id.vars = c('TIME', 'year', 'month')) %>%
  ggplot(aes(x = value)) +
  geom_density(color = 'blue') +
  facet_wrap(~variable)
```

*2. Yearly maxima of ADCIRC POST Detided*

```{r create yearly maxima time-series, message = F}
# create data frame with yearly maxima for each station
 df_max <- ADC_POST_Detided_region3 %>% 
  select(-TIME, -month) %>%
  group_by(year) %>%
  summarise_all(max, na.rm = T)

# change year from class character to numeric
df_max <- df_max %>%
  mutate(year = as.numeric(year))

# single station
df_max %>%
  select(year, WrightsvilleBeach) %>%
  ggplot(aes(x = year, y = WrightsvilleBeach)) +
  geom_point(col = 'blue')

# correlation plot
df_max %>%
  select(2:8) %>%
  ggpairs(
    upper = list(continuous = wrap("density", alpha = 0.5), combo = "box"),
    lower = list(continuous = wrap('points', size = 0.3))
  )

tibble(cor = sort(cor(df_max[, -1]))) %>%
  filter(cor < 1) %>% # exclude diagnoal elements or correlation matrix
  mutate(n = 1:n()) %>%
  ggplot(aes(x = n, y = cor)) +
  geom_point(size = 0.1)


# visualize the block-maxima through time-seires, box plots, violin plots, histograms and density plots.
df_max %>%
  as.tibble() %>%
  melt(id.vars = c('year')) %>%
  ggplot(aes(x = year, y = value)) +
  geom_line(col = 'blue') +
  facet_wrap(~variable)

df_max %>%
  select(-year) %>%
  melt() %>%
  ggplot(aes(x = value, y = variable)) +
  geom_boxplot(outlier.size = 0.01) 

df_max %>%
  as.tibble() %>%
  melt(id.vars = c('year')) %>%
  ggplot(aes(x = value)) +
  geom_density(col = 'blue') +
  facet_wrap(~variable)

# variogram of yearly maxima
df_max %>% 
  select(-year) %>% 
  as.matrix() %>%
  variogram(as.matrix(df_meta_region3[, c(3,4)]))
```


```{r create yearly maxima time-series with NA values, message = F}
# compute block maxima for given station using only those years which the proportion of NAs are < threshold
max_timeseries <- function(timeseries, threshold) {
  
  timeseries_na <- overview_na(timeseries, threshold, T)
  results <- list()
  
for(station in colnames(timeseries[, 4:ncol(timeseries)])) {
  
  df <- timeseries %>%
      select(year, station) %>%
      filter(!year %in% timeseries_na$station) %>%
      drop_na() %>%
      group_by(year) %>%
      summarise(max = max(eval(as.name(station))))
  
  results[[station]] <- df
}
  return(results)
}

ADC_POST_Detided_max <- max_timeseries(ADC_POST_Detided_region3, threshold = 0.1)
NOAA_OBS_Detided_max <- max_timeseries(NOAA_OBS_Detided_region3, threshold = 0.1)
```

### Statistical Modeling

*First Stage of Inference: Find the MLEs of the GEV parameters*

```{r}
gev_results <- list() # list to store independent GEV fits for each station

# fit and store the models in the prepared list
for(i in 2:ncol(df_max)) {
  gev_results[[i-1]] <- fevd(df_max[[i]], use.phi = T, type = 'GEV') # phi = T means log(sigma) used instead of sigma
}

names(gev_results) <- colnames(df_max)[-1] # add station names to results

# compute and store the gev parameter MLEs
par_results <- list()

for(i in 1:length(gev_results)) {
  
  par_results[[i]] <- gev_results[[i]]$results$par
  
}

names(par_results) <- colnames(df_max)[-1] 

# compute and store the estimated co-variance matrices
cov_results <- list()

for(i in 1:length(gev_results)) {
   
  cov_results[[i]] <- solve(gev_results[[i]]$results$hessian)
  
}

names(cov_results) <- colnames(df_max)[-1]

# compute and store the 20-year return levels
gev_return_lvls <- vector() 

for(i in 1:length(gev_results)) {
  
  gev_return_lvls[i] <- return.level(gev_results[[i]], return.period = 20)[1]
  
}

names(gev_return_lvls) <- colnames(df_max)[-1]

par(mfrow = c(4, 7)) # specifies the plot grid dimensions

# plot the return level plots for each station
for(i in 1:length(gev_results)) {
plot(gev_results[[i]], type = 'rl', main = colnames(df_max)[i+1], col = 'blue')
}
```

*Second Stage of Inference: d-variate Guassian process model*

Specify the likelihood of the Gaussian process

```{r specify gaussian process likelihood}
# number of spatial locations
L <- nrow(df_meta_region3)
# dimension of spatial process
d <- 3

# compute data vector of MLEs for gev parameters
theta_hat <- vector()

for(j in 1:d) {
  
  for(i in 1:L) {

  theta_hat <- c(theta_hat, par_results[[i]][j])

  }
  
}

# construct block diagonal variance matrix for the measurement error process
W <- bdiag(cov_results) # 3 parameters and 26 stations therefore 78 by 78 marix

# computes estimate of shortest distance (in meters) between two points on an ellipsoid (i.e. geodesic)
dist_matrix <- df_meta_region3 %>%
  select(lon, lat) %>%
  slice(26:1) %>%
  distm()

rownames(dist_matrix) <- colnames(df_max[, -1])
colnames(dist_matrix) <- colnames(df_max[, -1])

dim(dist_matrix)

# convert lat-lon to point in R^3
R <- 6371 # approximate radius of the earth in km

# compute x, y, z coordinates for each lat-lon pair
df_meta_region3 %>%
  mutate(x = R*cos(lat)*cos(lon), y = R*cos(lat)*sin(lon), z = R*sin(lat))


sum(1:26) - 26 == choose(26, 2) # number of distinct  location pairs

# compute variance matrix for spatially correlated random effects via Russel et al formula
S <- function(A, rho, dist) {
  
  # makes a p dimensional basis vector for coordinate
  make_basis <- function(k, p) replace(numeric(p), k, 1)
  
  storage <- list()
  
  for(i in 1:d) {
    
    N <- make_basis(i, 3) %*% t(make_basis(i, 3))
    
    O <- diag(L)
    
      for(j in 1:L) {
        
          for(k in 1:L) {
            
            O[j, k] <- exp(-dist[j, k]/rho[i])
              
          }
    }
      
    storage[[i]] <- kronecker(N, O)
    
  }
  
  V <- Reduce('+', storage)
  
  result <- kronecker(diag(L), A) %*% V %*% t(kronecker(diag(L), A))
  
  return(result)
  
}

# par = parameters, W = error covariance, dist = station by station distances
gaussian_process_likelihood <- function(par, data,  W, dist) {

# co-regionalization matrix used to define S below (i.e. S := Sigma_rho_A) 
A <- matrix(c(par[4], rep(0, 2),
              par[5:6], rep(0, 1),
              par[7:9]), byrow = T, nrow = 3, ncol = 3)

S <- S(A, par[10:12], dist)

# compute centered data term in likelihood
mu <- c(rep(par[1], L), rep(par[2], L), rep(par[3], L))
Z <- data - mu
  
# compute the log-likelihood
result <- -0.5*log(det(2*pi*(S + W))) - 0.5*t(Z)%*%solve(S + W)%*%Z
  
return(as.numeric(result))

}

# par = parameters, W = error covariance, dist = station by station distances
gaussian_process_likelihood_bulit_in <- function(par, data,  W, dist) {

# co-regionalization matrix used to define S below (i.e. S := Sigma_rho_A) 
A <- matrix(c(par[4], rep(0, 2),
              par[5:6], rep(0, 1),
              par[7:9]), byrow = T, nrow = 3, ncol = 3)

S <- S(A, par[10:12], dist)

# compute centered data term in likelihood
mu <- c(rep(par[1], L), rep(par[2], L), rep(par[3], L))
  
# compute the log-likelihood
result <- dmvnorm(data, mean = mu, sigma = S + W, log = T)
  
return(as.numeric(result))

}

# MVN density specified correctly
set.seed(10)
gaussian_process_likelihood(c(runif(9, 0.1, 100), 900, 800, 2000), data = theta_hat, W = W, dist_matrix/1000)
set.seed(7)
gaussian_process_likelihood_bulit_in(c(runif(9, 0.1, 100), 900, 800, 2000), data = theta_hat, W = as.matrix(W), dist_matrix/1000)
```

Find the MLEs of the parameter vector numerically.

```{r optimize likelihood}
# find MLEs
start_time <- Sys.time()

test1 <- optim(c(runif(p - d, 0.1, 100), runif(d, 500, max(dist_matrix))), gaussian_process_likelihood, data = theta_hat, W = W, dist = dist_matrix/1000, method = 'BFGS', hessian = T, control=list(maxit = 10000, fnscale = -1))

end_time <- Sys.time()
run_time <- end_time-start_time

run_time

test2 <- optim(c(runif(9, 0.1, 100), 600, 4000, 1000), gaussian_process_likelihood, data = theta_hat, W = W, dist = dist_matrix/1000, method = 'BFGS', hessian = T, control=list(maxit = 10000, fnscale = -1))


test1$par
test2$par

test1$par
gaussian_process_fit$par
test2
```

