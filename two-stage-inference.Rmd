---
title: "Guassian Process Model"
author: "Brian N. White"
date: "4/18/2022"
output: html_document
---

```{r, set global code chunk options, include = F}
knitr::opts_chunk$set(fig.align = 'center', warning = F, message = F, tidy = TRUE) # centers figures generated by code chunks
```

```{r load packages}
library(tidyverse) # data manipulation meta-package
library(Matrix) # for use in computing Kronecker product
library(mvtnorm) # compute MVN density
library(extRemes) # compute MLE of GEV model
library(geosphere) # used to compute geodesic distance between lat-lon pairs
library(gstat) # for (universal) kriging
library(sp) # for manipulating spatial data
```

```{r import data}
# data covers FEMA region 3
df_meta_region3 <- read.csv('data/df_meta_region3.csv') # lat-lon data
ADC_POST_Detided_region3 <- read.csv('data/ADC_POST_Detided_region3.csv') #  modeled (ADCIRC) data
NOAA_OBS_Detided_region3 <- read.csv('data/NOAA_OBS_Detided_region3.csv') # observed (NOAA) data
```

```{r clean data}
source('./functions/clean_timeseries.R') # converts time to POSIX class, adds variables month & year, replace station ID with station name
source('./functions/clean_metadata.R') # remove white space from station names

df_meta_region3 <- clean_metadata(df_meta_region3) # remove white space from station names


ADC_POST_Detided_region3 <- clean_timeseries(timeseries = ADC_POST_Detided_region3, 
                                             metadata = df_meta_region3 
                                                            %>% filter(stationname != 'Lewisetta')
                                             )

NOAA_OBS_Detided_region3 <- clean_timeseries(timeseries = NOAA_OBS_Detided_region3, metadata = df_meta_region3) %>%
                              select(-Lewisetta) %>% # exclude Lewisetta for consistency with ADCIRC data
                              select(c(1:3, 29:4)) # re-arrange stations to match order of columns in ADC_POST_Detided_region3

df_meta_region3 <- df_meta_region3 %>% 
                          filter(stationname != 'Lewisetta') # remove Lewisetta from meta data
```

```{r compute yearly maxima for each station}
# create data frame with yearly maxima for each station
 df_max <- ADC_POST_Detided_region3 %>% 
  select(-TIME, -month) %>%
  group_by(year) %>%
  summarise_all(max, na.rm = T) %>%
  mutate(year = as.numeric(year))
```

```{r 1st stage of inference: station-by-station non-stationary GEV fit}
gev_results <- list() # list to store independent GEV fits for each station

# fit and store the models in the prepared list
for(i in 2:ncol(df_max)) {
  gev_results[[i-1]] <- fevd(df_max[[i]], use.phi = T, type = 'GEV') # phi = T means log(sigma) used instead of sigma
}

names(gev_results) <- colnames(df_max)[-1] # add station names to results

# compute and store the GEV parameter MLEs
par_results <- list()

for(i in 1:length(gev_results)) {
  
  par_results[[i]] <- gev_results[[i]]$results$par
  
}

names(par_results) <- colnames(df_max)[-1] # add station names to results

# compute and store the estimated co-variance matrices
cov_results <- list()

for(i in 1:length(gev_results)) {
   
  cov_results[[i]] <- solve(gev_results[[i]]$results$hessian)
  
}

names(cov_results) <- colnames(df_max)[-1] # add station names to results
```

```{r specify model hyper parameters: L, d, and p}
L <- nrow(df_meta_region3) # number of spatial locations

d <- 3 # dimension of spatial process

# total number of parameters in model to be estimated
p <- 2*d + d*(d + 1)/2 # first term includes the mean and range parameters, second term the co-regionalization parameters

# examine growth of parameter space dimension as the dimension of the spatial process increases
p_function <- function(x) 2*x + x*(x + 1)/2

tibble(x = 1:20, y = p_function(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line(size = 0.3, col = 'blue') +
  labs(x = 'Dimension of Gaussian process', y = '# of parameters') +
  scale_y_continuous(breaks = seq(0, 250, by = 25)) 
```

```{r compute data vector: theta_hat}
# compute data vector of MLEs for gev parameters
theta_hat <- vector()

for(i in 1:d) { # each spatial process
  
  for(j in 1:L) { # each location

  theta_hat <- c(theta_hat, par_results[[j]][i])

  }
  
}
```

```{r compute distance matrix: dist_matrix}
dist_matrix <- df_meta_region3 %>%
  select(lon, lat) %>%
  slice(26:1) %>% # re-arrange stations to match order of parameters fit by fevd
  distm(fun = distHaversine)/(1000) # great-circle-distance via haversine method, assumes spherical earth

rownames(dist_matrix) <- colnames(df_max[, -1])
colnames(dist_matrix) <- colnames(df_max[, -1])
```

```{r compute variance matrix for measurement error process: W}
W <- bdiag(cov_results) %>% as.matrix()
```

```{r compute variance matrix for spatially correlated random effects: S}
S <- function(A, rho, dist) {
  
  # makes a p dimensional basis vector for component k
  make_basis <- function(k, p) replace(numeric(p), k, 1)
  
  storage <- list()
  
  for(i in 1:d) { # for each spatial process
    
    N <- make_basis(i, d) %*% t(make_basis(i, d))
    
    O <- diag(L)
    
      # for each location pair
      for(j in 1:L) {
        
          for(k in 1:L) {
            
            O[j, k] <- exp(-dist[j, k]/rho[i])
              
          }
    }
      
    storage[[i]] <- kronecker(N, O)
    
  }
  
  V <- Reduce('+', storage)
  
  result <- kronecker(diag(L), A) %*% V %*% t(kronecker(diag(L), A))
  
  return(result)
  
}
```

```{r compute Gaussian process likelihood}
# par = parameters, W = error covariance, dist = station by station distances
gaussian_process_likelihood <- function(par, data,  W, dist) {

# co-regionalization matrix used to define S below 
A <- matrix(0, nrow = d, ncol = d)
A[upper.tri(A, diag = TRUE)] <- par[(d + 1):(p - d)]
A <- t(A)

# variance matrix for spatially correlated random effects
S <- S(A, par[(p - d + 1):p], dist)

# mean vector  of the MVN density
mu <- vector()
for(i in 1:d) { # for each spatial process
  mu <- c(mu, rep(par[i], L))
}

# variance matrix of the MVN density
V <- S + W 
  
# MVVN negative log-likelihood
log_lik <- -0.5*log(det(2*pi*V)) - 0.5*t(data - mu)%*%solve(V)%*%(data - mu)
result <- -log_lik

# alternatively, use MVN density from mvtnorm package
# result <- -dmvnorm(data, mean = mu, sigma = S + W, log = T)
  
return(as.numeric(result))

}
```

```{r 2nd stage of inference: given theta_hat and W; find beta, A and rho}
start_time <- Sys.time() # track run time of optim

# fit p dimensional Gaussian process with assumed form of W
gaussian_process_fit <-  optim(c(runif(p - d, 0.1, 100), runif(d, 500, max(dist_matrix))), 
                               gaussian_process_likelihood, 
                               data = theta_hat, W = W, 
                               dist = dist_matrix, 
                               method = 'BFGS', 
                               hessian = T, 
                               control=list(maxit = 10000))

end_time <- Sys.time()
run_time <- end_time-start_time

run_time

# extract the mean parameter values over R^2
beta <- gaussian_process_fit$par[1:d]

# extract A, the co-regionalization  lower diagonal matrix
A <- matrix(0, nrow = d, ncol = d)
A[upper.tri(A, diag = TRUE)] <- gaussian_process_fit$par[(d + 1):(p - d)]
A <- t(A)

# extract range parameters
rho <- gaussian_process_fit$par[(p - d + 1):p]

# extract covariance matrix
cov <- solve(gaussian_process_fit$hessian)

# organize results
gaussian_results <- list(beta, A, rho, cov)
names(gaussian_results) <- c('beta: mean parameter values over R^2', 
                             'A: co-regionalization matrix', 
                             'rho: range parameters', 
                             'covariance matrix')

gaussian_results
format(sqrt(diag(gaussian_results$`covariance matrix`)), scientific = F) # compute mle sdevs

# examine eigen-values of fitted gaussian process covariance matrix
eigen(S(gaussian_results$`A: co-regionalization matrix`, gaussian_results$`rho: range parameters`, dist_matrix) + W)$values
```

```{r implement kriging}
# theta_hat is output from 1st stage of inference, par is output from second stage, W is error process covariance
kriging_pred <- function(new_loc, theta_hat, rho, A, W) {
  
  x0 <- matrix(rep(1, d), nrow = d, ncol = 1) # new location design vector
  X <- rep(1, d*L) # design matrix
  
  V <- S(A, rho, dist_matrix)
  sigma_inv <- solve(V + W) # plug-in MLE estimate of covariance matrix for theta hat
  
  # beta_hat <- solve(t(X) %*% sigma_inv %*% X) %*% t(X) %*% sigma_inv %*% theta_hat # GLS estimate of mean
  beta_hat <- gaussian_results$`beta: mean parameter values over R^2`
  
  tau <- matrix(rep(0, d*L), nrow = d, ncol = d*L) # compute covariance of new location and theta hat
  
  storage <- list()
  
   for(i in 1:L) { # for each location
     
     C <- diag(d) # covariance matrix for new obs and location i
     
     for(j in 1:d) { # for each spatial process
       
       C[j, j] <- exp(-distHaversine(new_loc, df_meta_region3[27 - i, 4:3])*(10^-3)/rho[j])
       
     }
     
     storage[[i]] <-  A %*% C %*% t(A)
     
   }
  
  tau <- Reduce(cbind, storage) # covariance matrix of new location with existence locations
  
  pred <- beta_hat + tau %*% sigma_inv %*% W %*% (theta_hat - c(rep(beta_hat[1], 26), rep(beta_hat[2], 26), rep(beta_hat[3], 26)))
  # pred <- t(theta_hat) %*% sigma_inv %*% t(tau)
  # pred <- x0 %*% beta_hat + tau %*% sigma_inv %*% (theta_hat - X %*% beta_hat), original
  # pred <- x0 %*% beta_hat + tau %*% sigma_inv %*% W  %*% (theta_hat - X %*% beta_hat)
   
  cov <- A %*% t(A) - tau %*% sigma_inv %*% t(tau)
    
  rownames(pred) <- c('location', 'log.scale', 'shape')
  
  result <- list(pred, cov)
  names(result) <- c('predictions', 'covariance')
  
  return(result)

}
```

```{r compute kriging predictions and covariances}
df_meta_region3 %>%
  select(stationname, lon, lat)

check <- list()

for(i in 1:L) {
  
check[[i]] <- kriging_pred(df_meta_region3[27 - i, 4:3], 
              theta_hat, 
              gaussian_results$`rho: range parameters`, 
              gaussian_results$`A: co-regionalization matrix`,
              W)

}
  

names(check) <- colnames(df_max[,-1])

for(i in 1:L) {
  
  print(check[i])
  print(par_results[i])
  
}
```

```{r implement return levels}
return_levels <- function(par, cov, p) {
  
  scale <- exp(par[2])
  
  y_p <- -log(1 - p) # 1/p return level, where p is the upper tail probability: see Coles (2001) page 56,
  
  if ( abs(par[3]) > 0.001 ) { # if the shape parameter is not about zero
    
  z_p <- par[1] - (scale/par[3])*(1 - y_p^(-par[3]))
  
  } else { # if the she parameter is about zero
    
  z_p <- par[1] - scale*log(y_p)
    
  }
  
  # compute variance for z_p
  
    # compute gradient of z_p via delta-method approximation
    z_p_grad <- c(1, 
               -par[3]^(-1)*(1 - y_p^-par[3]), 
                scale*par[3]^(-2)*(1 - y_p^-par[3]) - scale*par[3]^(-1)*y_p^(-par[3])*log(y_p))
  
    z_p_var <- t(z_p_grad) %*% cov %*% z_p_grad

 result <- list(z_p, z_p_var)
 names(result) <- c('1/p return level', 'variance')
 
 return(result)
  
}
```

```{r compute return levels and variances}
rl_check <- list()

for(i in 1:L) {
  
  rl_check[[i]] <- return_levels(check[[i]][[1]], check[[i]][[2]], 1/100)
    
}

names(rl_check) <- colnames(df_max[, -1])


for(i in 1:L) {
print(rl_check[[])
}

plot()
```
