---
title: "two-stage-inference-simple"
author: "Brian N. White"
date: "2022-05-03"
output: html_document
---

```{r, set global code chunk options, include = F}
knitr::opts_chunk$set(fig.align = 'center', warning = F, message = F, tidy = TRUE) # centers figures generated by code chunks
```

```{r load packages}
library(tidyverse) # data manipulation meta-package
library(Matrix) # for use in computing Kronecker product
library(mvtnorm) # compute MVN density
library(extRemes) # compute MLE of GEV model
library(geosphere) # used to compute geodesic distance between lat-lon pairs
library(numDeriv)
```

```{r import data}
# data covers FEMA region 3
df_meta_region3 <- read.csv('data/df_meta_region3.csv') # lat-lon data
ADC_POST_Detided_region3 <- read.csv('data/ADC_POST_Detided_region3.csv') #  modeled (ADCIRC) data
NOAA_OBS_Detided_region3 <- read.csv('data/NOAA_OBS_Detided_region3.csv') # observed (NOAA) data
```

```{r clean data}
source('./functions/clean_timeseries.R') # converts time to POSIX class, adds variables month & year, replace station ID with station name
source('./functions/clean_metadata.R') # remove white space from station names

df_meta_region3 <- clean_metadata(df_meta_region3) # remove white space from station names


ADC_POST_Detided_region3 <- clean_timeseries(timeseries = ADC_POST_Detided_region3, 
                                             metadata = df_meta_region3 
                                                            %>% filter(stationname != 'Lewisetta')
                                             )

NOAA_OBS_Detided_region3 <- clean_timeseries(timeseries = NOAA_OBS_Detided_region3, metadata = df_meta_region3) %>%
                              select(-Lewisetta) %>% # exclude Lewisetta for consistency with ADCIRC data
                              select(c(1:3, 29:4)) # re-arrange stations to match order of columns in ADC_POST_Detided_region3

df_meta_region3 <- df_meta_region3 %>% 
                          filter(stationname != 'Lewisetta') # remove Lewisetta from meta data
```

```{r compute yearly maxima for each station}
# create data frame with yearly maxima for each station
 df_max <- ADC_POST_Detided_region3 %>% 
  select(-TIME, -month) %>%
  group_by(year) %>%
  summarise_all(max, na.rm = T) %>%
  mutate(year = as.numeric(year))
```

```{r 1st stage of inference: station-by-station non-stationary GEV fit}
gev_results <- list() # list to store independent GEV fits for each station

# fit and store the models in the prepared list
for(i in 2:ncol(df_max)) {
  gev_results[[i-1]] <- fevd(df_max[[i]], use.phi = T, type = 'GEV') # phi = T means log(sigma) used instead of sigma
}

names(gev_results) <- colnames(df_max)[-1] # add station names to results

# compute and store the GEV parameter MLEs
par_results <- list()

for(i in 1:length(gev_results)) {
  
  par_results[[i]] <- gev_results[[i]]$results$par
  
}

names(par_results) <- colnames(df_max)[-1] # add station names to results

# compute and store the estimated variance matrices
cov_results <- list()

for(i in 1:length(gev_results)) {
   
  cov_results[[i]] <- solve(gev_results[[i]]$results$hessian)
  
}

names(cov_results) <- colnames(df_max)[-1] # add station names to results
```

```{r compute the data vector}
B <- matrix(rep(0, 3*26), nrow = 3, ncol = 26)

for(i in 1:3) {
  
  for(j in 1:26) {
    
    B[i, j] <- par_results[[j]][i]
    
  }
  
}

rownames(B) <- c('loc', 'log scale', 'shape')
colnames(B) <- colnames(df_max[, -1])

# data vectors for second stage of inference: each vector corresponds to a uni-variate Gaussian process
theta_hat_loc <- B[1, ] 
theta_hat_scale <- B[2, ]
theta_hat_shape <- B[3, ]
```

```{r compute distance matrix: dist_matrix}
dist_matrix <- df_meta_region3 %>%
  select(lon, lat) %>%
  slice(26:1) %>% # re-arrange stations to match order of parameters fit by fevd
  distm(fun = distHaversine)/1000 # great-circle-distance via haversine method, assumes spherical earth, divide by 1000 to get km

rownames(dist_matrix) <- colnames(df_max[, -1])
colnames(dist_matrix) <- colnames(df_max[, -1])
```

```{r compute variance matrix for measurement error process: W}
# construct variance matrices for measurement error process in special case of independent univariate Gaussian processes
C  <- matrix(rep(0, 3*26), nrow = 3, ncol = 26) 
# each row corresponds to a gev parameter estimate and contains the  corresponding variance at each location

for(i in 1:3) {
  
  for(j in 1:26) {
  
C[i, j] <- cov_results[[j]][i, i]
  
  }
  
}

rownames(C) <- c('loc var', 'log scale var', 'shape var')
colnames(C) <- colnames(df_max[, -1])

# variance matrices for measurement error process for univariate Gaussian processes
W_loc <- diag(C[1,])
W_scale <- diag(C[2,])
W_shape <- diag(C[3,])
```

```{r construct variance matrix for spatially correlated random effect: V}
V <- function(dist, rho) {

  Q <- matrix(rep(0, 26), nrow = 26, ncol = 26)
    
    # for all location pairs compute exponential covariance
    for(i in 1:26) { 
  
      for(j in 1:26) {
    
        Q[i, j] <- exp(-dist[i, j]/rho)
    
    }
    
    }
  
  return(Q)

}
```

```{r compute gaussian process likelihood}
gaussian_process_likelihood_simple <- function(par, data, W, dist) {
  
  # variance matrix for spatially correlated random effects
  V <- V(dist, par[2])

  # mean vector  of the MVN density
  mu <- rep(par[1], 26)
  
  # MVN negative log-likelihood
  log_lik <- -0.5*log(det(2*pi*(V + W))) - 0.5*t(data - mu) %*% solve(V + W) %*% (data - mu)
  result <- -log_lik
  
  # using mvtnorm
  # result <- -dmvnorm(data, mean = mu, sigma = V + W, log = T)
  
  return(result)
  
}
```

```{r 2nd stage of inference: given theta_hat and W; find beta and rho}
# fit independent uni-variate Gaussian processes for each parameter from the first stage of inference
gaussian_process_fit_loc <-  optim(c(0.1, runif(1, 500, max(dist_matrix))),
                               gaussian_process_likelihood_simple, 
                               data = theta_hat_loc,
                               W = W_loc, 
                               dist = dist_matrix, 
                               method = 'Nelder-Mead', 
                               hessian = T, 
                               control=list(maxit = 10000))

gaussian_process_fit_scale <-  optim(c(20, runif(1, 500, max(dist_matrix))), 
                               gaussian_process_likelihood_simple, 
                               data = theta_hat_scale, 
                               W = W_scale, 
                               dist = dist_matrix, 
                               method = 'Nelder-Mead', 
                               hessian = T, 
                               control=list(maxit = 10000))


gaussian_process_fit_shape <-  optim(c(0.5, runif(1, 500, max(dist_matrix))), 
                               gaussian_process_likelihood_simple, 
                               data = theta_hat_shape, 
                               W = W_shape, 
                               dist = dist_matrix, 
                               method = 'Nelder-Mead', 
                               hessian = T, 
                               control=list(maxit = 10000))


gaussian_process_fit_loc
sqrt(diag(solve(gaussian_process_fit_loc$hessian)))

gaussian_process_fit_scale
sqrt(diag(solve(gaussian_process_fit_scale$hessian)))

gaussian_process_fit_shape
sqrt(diag(solve(gaussian_process_fit_shape$hessian)))


# compute alpha_hat

alpha_hat <- function(theta_hat, par, W) {
  
    X <- rep(1, 26)
    
    V <- V(dist_matrix, par[2])
    sigma_inv <- solve(V + W)
    
    beta_hat <- solve(t(X) %*% sigma_inv %*% X) %*% t(X) %*% sigma_inv %*% theta_hat
    
    G_sqr <- t(theta_hat - X %*% beta_hat) %*% sigma_inv %*% (theta_hat - X %*% beta_hat)
    
    return(as.numeric(G_sqr/26))
  
}

alpha_hat_loc <- alpha_hat(theta_hat_loc, gaussian_process_fit_loc$par, W_loc)
alpha_hat_scale <- alpha_hat(theta_hat_scale, gaussian_process_fit_scale$par, W_loc)
alpha_hat_shape <- alpha_hat(theta_hat_shape, gaussian_process_fit_shape$par, W_loc)
```


```{r implement kriging}
kriging_pred <- function(new_loc, theta_hat, par, alpha_hat, W) {
  
  x0 <- 1
  
  V <- alpha_hat*V(dist_matrix, par[2])
  sigma_inv <- solve(V + W)
  
  tau <- rep(0, 26)
    
    for(i in 1:26) {
      tau[i] <- alpha_hat*exp(-distHaversine(new_loc, df_meta_region3[27 - i, 4:3])*(10^-3)/par[2]) # index scheme is so        that stations are in the correct order (i.e. match theta_hat station order)
    }
  
  beta_hat <- par[1]
  
  pred <- beta_hat + t(tau) %*% sigma_inv %*% (theta_hat - rep(beta_hat, 26))
  var <- alpha_hat^2 - t(tau) %*% sigma_inv %*% tau
  
  sd <- sqrt(var)
  
  result <- c(pred, sd)
  
  names(result) <- c('prediction', 'sd')
    
  return(result)
  
}
```

```{r generate predictions at new locations}
# location process
check_loc <- list()

for(i in 1:26) {
  
check_loc[[i]] <- kriging_pred(df_meta_region3[27 - i, 4:3], 
              theta_hat_loc, 
              gaussian_process_fit_loc$par,
              alpha_hat_loc,
              W_loc)

}

names(check_loc) <- colnames(df_max[, -1])

for(i in 1:26) {
  
print(check_loc[i])

# print(par_results[[i]])

}


# log-scale process
check_scale <- list()

for(i in 1:26) {
  
check_scale[[i]] <- kriging_pred(df_meta_region3[27 - i, 4:3], 
              theta_hat_scale, 
              gaussian_process_fit_scale$par,
              alpha_hat_scale,
              W_scale)

}

names(check_scale) <- colnames(df_max[, -1])

for(i in 1:26) {
  
print(check_scale[i])

# print(par_results[[i]])

}

# shape process
check_shape <- list()

for(i in 1:26) {
  
check_shape[[i]] <- kriging_pred(df_meta_region3[27 - i, 4:3], 
              theta_hat_shape, 
              gaussian_process_fit_shape$par,
              alpha_hat_shape,
              W_shape)

}

names(check_shape) <- colnames(df_max[, -1])

for(i in 1:26) {
  
print(check_shape[i])

# print(par_results[[i]])

}

```

```{r implement return levels}
return_levels <- function(par, p) {
  
  y_p <- -log(1 - p) # 1/p return level, where p is the upper tail probability: see Coles (2001) page 56,
  
  if ( abs(par[3]) > 0.001 ) { # if the shape parameter is not about zero
    
  z_p <- par[1] - (par[2]/par[3])*(1 - y_p^(-par[3]))
  
  } else { # if the she parameter is about zero
    
  z_p <- par[1] - par[2]*log(y_p)
    
  }
  
  # compute variance for z_p
  
    # compute gradient of z_p via delta-method approximation
    # z_p_grad <- c(1, 
               # -par[3]^(-1)*(1 - y_p^-par[3]), 
               # par[2]*par[3]^(-2)*(1 - y_p^-par[3]) - par[2]*par[3]^(-1)*y_p^(-par[3])*log(y_p))
  
    # z_p_var <- t(z_p_grad) %*% cov %*% z_p_grad

 result <- list(z_p)
 names(result) <- c('1/p return level')
 
 return(result)
  
}
```

```{r compute 1/p return levels}
rl_check <- vector()

for(i in 1:26) {
  

rl_check[i] <- return_levels(as.numeric(c(check_loc[i], exp(as.numeric(check_scale[i])), check_shape[i])), 
              1/100)
  
}


names(rl_check) <- colnames(df_max[, -1])

# compare 100 year return level with known estimates
for(i in 1:26) {
  
print(rl_check[i])
print(return.level(gev_results[[i]], return.period = c(100))[1])

}
```

